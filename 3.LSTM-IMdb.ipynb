{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujw5h5Nm8z8s"
      },
      "source": [
        "<center>\n",
        "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1>Curso Procesamiento de Lenguaje Natural</h1>\n",
        "\n",
        "<h3>LSTM con Keras, un flujo básico pero completo</h3>\n",
        "\n",
        "\n",
        "<p> Kevin David Ruiz Gonzalez </p>\n",
        "<p>\n",
        "<img src=\"https://identidadbuho.unison.mx/wp-content/uploads/2019/06/letragrama-cmyk-72.jpg\" width=\"150\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/LSTM-IMdb.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
        "\n",
        "<p>\n",
        "Tomado parcialmente y adaptado de varias libretas de la documentación de Keras\n",
        "</p>\n",
        "\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "25m0QWwa8z8x"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVP45gG48z8y"
      },
      "source": [
        "# Obteniendo datos\n",
        "\n",
        "Vamos a recuperar la base de datos globera de IMdb que se usa para probar casi todos los modelos. Vamos a recuperar los adatos de\n",
        "\n",
        "``https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EAuL6qRn8z8y",
        "outputId": "57a3ff58-a9c0-4fe9-a1f1-61f1d434c844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5599k      0  0:00:14  0:00:14 --:--:-- 10.2M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdoapI6n8z8z"
      },
      "source": [
        " y vamos a investigas la estructura y lo que hay..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dTvW9WJV8z8z",
        "outputId": "c1304859-53c8-422b-b8c4-e0fcd067b1f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XexqZmIx8z8z",
        "outputId": "fdaecf4e-8683-4c6c-fb01-10a686b0bc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "76eIaZa08z80",
        "outputId": "83282092-b6a7-45cb-c820-4519f18c92f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labeledBow.feat  neg  pos  unsup  unsupBow.feat  urls_neg.txt  urls_pos.txt  urls_unsup.txt\n"
          ]
        }
      ],
      "source": [
        "!ls aclImdb/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pJXYZzbW8z80",
        "outputId": "cf6c868a-c3fb-4da7-d61d-e950d06f53d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ],
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNnFVFtZ8z80"
      },
      "source": [
        "Solo nos interesan las evaluaciones positivas y negativas (para hacer una simple clasificación binaria y simplificar la aplicación), por lo que vamos a borrar el folder `unsup`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4e7de-j88z81"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stigsi8F8z81"
      },
      "source": [
        "Ahora si, vamos a usar las librerías de `Keras` para leer los datos usando [`keras.utils.text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory).\n",
        "\n",
        "En este momento es donde tenemos que determinar el tamaño de los lotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1yYj5MjB8z81",
        "outputId": "7b70280d-479c-4371-9afc-6a97e1e28652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32         # Tamaño de los minibatches\n",
        "\n",
        "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rbN2oHje8z81",
        "outputId": "d6102f14-2919-4988-baf0-c1e5756c7052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de batches en raw_train_ds: 625\n",
            "Numero de batches en raw_val_ds: 157\n",
            "Numero de batches en raw_test_ds: 782\n"
          ]
        }
      ],
      "source": [
        "print(f\"Numero de batches en raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Numero de batches en raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Numero de batches en raw_test_ds: {raw_test_ds.cardinality()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLxPKJlQ8z82"
      },
      "source": [
        "Es importante revisar los datos crudos para tener una idea de como se recuperaron y cual es la forma que tienen.\n",
        "\n",
        "Esto lo podemos hacer tomando algunos datos de cada batch e imprimiendolos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WzwkoZuH8z82",
        "outputId": "b925e651-69e2-4a36-8c11-954bf525b8a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I am very disappointed with \"K-911.\" The original \"good\" quality of \"K-9\"\n",
            "> doesn\\'t exist any more. This is more like a sitcom! Some of casts from\n",
            "> original movie returned and got some of my memory back. The captain of Dooley\n",
            "> now loves to hit him like a scene from old comedy show. That was crazy.\n",
            "> What\\'s the deal with the change of Police? It seems like they are now LAPD!\n",
            "> Not San Diego PD. It is a completely different movie from \"'\n",
            "\n",
            "target = 0\n",
            "b\"Giallo fans, seek out this rare film. It is well written, and full of all\n",
            "> sorts of the usual low lifes that populate these films. I don't want to give\n",
            "> anything away, so I wont even say anything about the plot. The whole movie\n",
            "> creates a very bizarre atmosphere, and you don't know what to expect or who to\n",
            "> suspect. Recommended! The only place I've seen to get this film in english is\n",
            "> from European Trash Cinema, for $15.\"\n",
            "\n",
            "target = 1\n",
            "b\"Terry Gilliam's and David Peoples' teamed up to create one of the most\n",
            "> intelligent and creative science fiction movies of the '90's. People's proved\n",
            "> a screenplay with bizarre twists and fantastic ideas about the nature of time\n",
            "> \\xc2\\x97 I especially love the idea one can't change the past; it's a nice\n",
            "> counterpoint to so many time-travelling movies which say otherwise \\xc2\\x97\n",
            "> biological holocausts and the thin line between sanity and madness. Gilliam\n",
            "> visualized his ideas with unique quirkiness, perfection and originality.<br\n",
            "> /><br />The story itself is engaging: one man, James Cole (played by Bruce\n",
            "> Willis in a heart-warming performance) travels several decades to the past to\n",
            "> retrieve information about a virus that's wiped out mankind and left only a\n",
            "> few survivors alive living underground: with the information he'll collect,\n",
            "> scientists hope to find a cure so everyone in the future can return to the\n",
            "> surface. But because their time-travelling technology isn't perfect, he ends\n",
            "> up being sent towards different other pasts and complicating things. And from\n",
            "> that a brilliant science fiction thriller with shades of film noir ensues as\n",
            "> the multiple pieces of a huge jigsaw start fitting together to form a bizarre\n",
            "> narrative involving animal right activists, end of the millennium paranoia,\n",
            "> biological weapons, the perception of reality, and the definition of sanity.\n",
            "> With such a complex movie, it was easy for Gilliam and Peoples to create a\n",
            "> mess, but instead Twelve Monkeys is a thought-provoking narrative which will\n",
            "> please those who like to be challenged and have patience to appreciate some\n",
            "> crazy ideas.<br /><br />I watched this movie once around 10 years ago. It\n",
            "> marked me a lot: I remember still thinking about many days after-wards; for my\n",
            "> young mind this seemed quite mind-blowing and it was one of the first movies\n",
            "> to make me appreciate cinema as something serious and important. I've re-\n",
            "> watched this movie a few days ago on DVD and it's better than I remembered it.\n",
            "> Brad Pitt still steals all the scenes he's in, playing Jeffrey Goines \\xc2\\x97\n",
            "> almost a prelude to his Tyler Durden character in Fight Club \\xc2\\x97 a rich\n",
            "> kid with some anarchist/non-conformist ideas who's also crazy and, according\n",
            "> to Cole, perhaps responsible for the virus. The scenes between Jeffrey and\n",
            "> Cole in the madhouse are the best in the movie, Pitt's eyes, voice and quirky\n",
            "> mannerisms convince you he's really a crazy guy locked in a warped logic only\n",
            "> he understands. Pitt's Oscar nomination was well deserved! Surprising was also\n",
            "> Bruce Willis' performance: his I didn't remember very well, but it's beautiful\n",
            "> and full of sensibility; he plays a man who spent almost all his life\n",
            "> underground, and when he comes to the past you'll share his childish\n",
            "> fascination with something as simple as breathing the fresh air of the morning\n",
            "> or watching the sun go up. Cole is a rather ambiguous character, Peoples'\n",
            "> tried to imbue some darkness in him, and he does other disturbing things to\n",
            "> other people and to himself: the scene where he removes his own teeth reveals\n",
            "> how far his dementia has gone unchecked. Ironically Cole didn't start as a\n",
            "> crazy character, but when he starts warning everyone about the end of the\n",
            "> world, he's considered mad and convinced it's all in his mind, until he\n",
            "> arrives at a point when he can't distinguish past from future, reality from\n",
            "> fiction. Willis spends a lot of time looking confused and insecure, and it\n",
            "> works perfectly. One of the fun twists in the narrative is when Cole's shrink,\n",
            "> Dr. Kathryn Railly, finds undeniable proof he's really from the future and now\n",
            "> has to convince him again of his mission to save the world. The screenplay is\n",
            "> full with weird twists like this and it keeps the movie in a fast pace. Their\n",
            "> relationship is also well-handed, although perhaps a bit compressed for time's\n",
            "> sake. But I enjoyed watching Cole and Railly falling in love and trying to\n",
            "> escape the authority of the future to live a peaceful life in the past. But\n",
            "> then things end in a tragic/bittersweet climax at an airport, wrapping all the\n",
            "> pieces together, which will blow many minds away.<br /><br />There are two\n",
            "> great endings in this movie, a twist in the sense of Se7en or Fight Club, and\n",
            "> a more intimate ending where Railly is crouching next to Cole who's just been\n",
            "> shot and looking around for a younger James Cole who's witnessing his future\n",
            "> self die; the two share a brief look, and she smiles at him. The twist is\n",
            "> brilliant, but I prefer this ending for emotional impact. Madeleine Stowe is\n",
            "> very good playing Dr. Railly, she drew many different emotions from me in her\n",
            "> performance. The movie is filled with a sense of fatalism with the idea the\n",
            "> past can't be changed: this movie shows that in a terrifying way. It reminds\n",
            "> me of Chinatown in that sense, the way Jake Gittes messes everything up the\n",
            "> more he tries to help. Railly's character shares that fatalism, the more she\n",
            "> tries to help Cole \\xc2\\x97 first dealing with his 'madness' then helping him\n",
            "> in his mission \\xc2\\x97 the more they're sucked into tragedy.<br /><br />The\n",
            "> twist ends with a hopeful note, though, with the feeling Cole's mission hasn't\n",
            "> been in vain. Twelve Monkeys is a great movie to watch if one wants to be\n",
            "> entertained; it's not supposed to be art, although it's more artists than many\n",
            "> artistic movies. It's an unpretentious movie where all elements, from music to\n",
            "> editing to costume design, etc., came together beautifully to produce a modern\n",
            "> cinema masterpiece.\"\n",
            "\n",
            "target = 1\n",
            "b\"We expected something great when we went to see this bomb. It is basically a\n",
            "> Broadway play put on film. The music is plain terrible. There isn't one\n",
            "> memorable song in the movie -- heard any hits from this movie? You won't\n",
            "> because there aren't any. Some of the musical numbers go on so long that I got\n",
            "> up to go to the restroom and get some pop corn and it was still going when I\n",
            "> got back! If they were good songs well -- but they suck. The pace is slow,\n",
            "> terrible character development. The lead was praised for her singing but\n",
            "> sounded like she screamed every song -- it was almost impossible to stand.\n",
            "> This movie has NOTHING to offer anyone but die-hard Broadway enthusiasts. This\n",
            "> is without a doubt the most over rated movie I've seen in my entire life. A\n",
            "> complete waist of time and money. There is nothing memorable about this movie\n",
            "> except Danny Glover -- who wasn't on screen enough and whose character wasn't\n",
            "> developed enough. Rent the video and you'll agree -- this movie was an\n",
            "> expensive, over produced, polished dog do.\"\n",
            "\n",
            "target = 0\n",
            "b'I understand this movie was made on a very low budget but that is no excuse\n",
            "> for the monstrosity that is Grendel. Deathstalker, The Throne of Fire,\n",
            "> Barbarian Queen, Conquest, the Invincible Barbarian were all done on\n",
            "> shoestring budgets and poor special effects yet they still managed to create\n",
            "> cult classics by adding some scantily clad women warriors and a good sense of\n",
            "> humor. The primitive costumes, dark castles and beautiful Bulgarian landscape\n",
            "> gave Grendel the potential to be a very good low budget sword and sorcery\n",
            "> film, but the makers completely ruined this opportunity by using extremely\n",
            "> poor CGI effects and colorless characters. Compare this film to Beowulf\n",
            "> (1999). It may not be Citizen Kane but it is a good example of how an\n",
            "> entertaining low budget sci-fi/ adventure movie can be made by using credible\n",
            "> special effects and appealing characters.'\n",
            "\n",
            "target = 0\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(textwrap.fill(str(text_batch.numpy()[i]), 80, subsequent_indent='> '))\n",
        "        print(\"\\ntarget =\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnj2RxEx8z82"
      },
      "source": [
        "## Preparando los datos\n",
        "\n",
        "Vamos ahora a convertir cada string de datos en una serie de índices numéricos, los cuales puedan entrar en\n",
        "un modelo neuronal. Para esto, vamos a generar índices a partir de las palabras existentesd en el texto.\n",
        "\n",
        "Este métdo puede ser no el mejor, ya que el vocabulario se fija en relación al vocabulario encontrado en el\n",
        "conjunto de aprendizaje. Más adelante veremos mñetodos más sofisticados para hacer la indezación, o como\n",
        "usar un vocabulario indexado ya preestablecido.\n",
        "\n",
        "Por el momento vamos primero a especificar el proceso de limpieza de texto (preprocesamiento) el cual será muy sencillo para este ejemplo y consiste en:\n",
        "\n",
        "1. Convertir a minúsculas todas las letras\n",
        "2. Eliminar los saltos de linea en formato *html* ( `<br /> `)\n",
        "3. Eliminar los signos de puntuación\n",
        "\n",
        "Igualmente, vamos a generar los minibatches con secuencias de `sequence_length` palabras. Esto es, si es insuficiente, se trunca el texto y si es\n",
        "demasiado, se completa el texto con 0's. De esa manera, todos los modelos aprenden con secuencias del mismo tamaño.\n",
        "\n",
        "Se utilizan hasta `max_features` tokens diferentes. De haber más, estos se eliminan en función de su frecuencia.\n",
        "\n",
        "Para esto vamos a utilizar la capa de `Keras` de [`layers.TextVectorization`](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IAST1iKt8z82"
      },
      "outputs": [],
      "source": [
        "# Model constants.\n",
        "max_features = 20000\n",
        "sequence_length = 500\n",
        "\n",
        "# Preprocesamiento\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Capa de vectorización (encontrar los índices por palabra)\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4Ncdqwdn8z82"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AfvrAZFU8z82"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3x0IpXqW8z83"
      },
      "outputs": [],
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OX4jHqHH8z83",
        "outputId": "8d4e16d1-57d3-45c4-d1a1-2988b73cf5f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Donde se guardan los datos de entrenamiento\n",
            "train_ds.cardinality() =  tf.Tensor(625, shape=(), dtype=int64)\n",
            "\n",
            "Y un minibatch se representa de esta manera: \n",
            "\n",
            "(<tf.Tensor: shape=(32, 500), dtype=int64, numpy=\n",
            "array([[   10,    19,  1692, ...,     0,     0,     0],\n",
            "       [    1,  1017,    19, ...,     0,     0,     0],\n",
            "       [   11,    38,  2925, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [   11,   203,    76, ...,     0,     0,     0],\n",
            "       [   10,     7,     2, ...,     0,     0,     0],\n",
            "       [    2, 11069,     7, ...,     0,     0,     0]])>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Donde se guardan los datos de entrenamiento\")\n",
        "print(\"train_ds.cardinality() = \", train_ds.cardinality())\n",
        "\n",
        "ejemplo = train_ds.take(1)\n",
        "\n",
        "print(\"\\nY un minibatch se representa de esta manera: \\n\")\n",
        "print(ejemplo.get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWYyQ_fK8z83"
      },
      "source": [
        "## Modelo basado en LSTM multicapa\n",
        "\n",
        "Vamos a hacer un modelo multicapa, el cual seguramente requerirá de ajustes de su parte.\n",
        "\n",
        "Vamos a utilizar la forma funcional de definir un modelo neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QDOKpclQ8z83",
        "outputId": "98a56a95-2f3e-4fc9-e9fb-389b17929710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2823297 (10.77 MB)\n",
            "Trainable params: 2823297 (10.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb = 128               # Embedding size\n",
        "unidades = 128          # Hidden units per layer\n",
        "\n",
        "\n",
        "# Entrada en indices\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "\n",
        "# Dos capas de LSTMs\n",
        "x = layers.LSTM(unidades, return_sequences=True)(x)\n",
        "x = layers.LSTM(unidades)(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs, predictions)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62JlakuG8z83"
      },
      "source": [
        "Compilamos y ponemos a aprender el modelo (usando BPTT en forma automñatica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NCII3kdQ8z83",
        "outputId": "1cd9c2ba-b6c2-4439-ac24-c2314d972771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 73s 108ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6927 - val_accuracy: 0.5048\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6839 - accuracy: 0.5215 - val_loss: 0.7110 - val_accuracy: 0.5070\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f02402d7820>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model.compile(\n",
        "    \"adam\",\n",
        "    \"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mcl61GC8z83"
      },
      "source": [
        "Y probamos con los datos de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EzFDkacc8z83",
        "outputId": "17449c9b-aeb9-4e0d-969f-ab0b81470cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 14s 18ms/step - loss: 0.7102 - accuracy: 0.5036\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7102249264717102, 0.5035600066184998]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlh11c1-8z84"
      },
      "source": [
        "Y ahora vamos a probar con bi-LSTM, haciendo un poco más complicado (aunque no mucho) el código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "URqGDF2i8z84",
        "outputId": "51b9170c-af5e-462e-a9d0-2b3c587849ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 256)         263168    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 256)               394240    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3250433 (12.40 MB)\n",
            "Trainable params: 3250433 (12.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb = 128\n",
        "unidades = 128\n",
        "\n",
        "# Input\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "\n",
        "# bi-LSTMs\n",
        "x = layers.Bidirectional(\n",
        "    layers.LSTM(unidades, return_sequences=True)\n",
        ")(x)\n",
        "x = layers.Bidirectional(\n",
        "    layers.LSTM(unidades)\n",
        ")(x)\n",
        "\n",
        "# Vanilla hidden layer:\n",
        "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model_bi = keras.Model(inputs, predictions)\n",
        "model_bi.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WN0gBC128z84",
        "outputId": "2d22999e-0fe5-4f22-f102-9e297c23a2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 96s 142ms/step - loss: 0.6511 - accuracy: 0.6007 - val_loss: 0.6137 - val_accuracy: 0.6402\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 50s 80ms/step - loss: 0.3713 - accuracy: 0.8387 - val_loss: 0.3081 - val_accuracy: 0.8742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f02ee055270>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model_bi.compile(\n",
        "    \"adam\",\n",
        "    \"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_bi.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "80Omh0vc8z84",
        "outputId": "40a69010-f06e-4a0c-bfa4-0d3524b34863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 23s 29ms/step - loss: 0.3210 - accuracy: 0.8646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3209541141986847, 0.8645600080490112]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model_bi.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnuKf6S8z84"
      },
      "source": [
        "## Modelo por convolucionales de 1 dimensión\n",
        "\n",
        "Este modelo viene como modelo de base en Keras, y es un buen inicio para ver como usar convolucionales como modelos para PLN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rJAruSLo8z84"
      },
      "outputs": [],
      "source": [
        "emb = 128\n",
        "unidades = 128\n",
        "ventana = 7\n",
        "drop= 0.5\n",
        "\n",
        "# Entrada\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Capa de embeddings\n",
        "x = layers.Embedding(max_features, emb)(inputs)\n",
        "x = layers.Dropout(drop)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(\n",
        "    unidades,\n",
        "    ventana,\n",
        "    padding=\"valid\",\n",
        "    activation=\"relu\",\n",
        "    strides=3\n",
        ")(x)\n",
        "x = layers.Conv1D(\n",
        "    unidades,\n",
        "    ventana,\n",
        "    padding=\"valid\",\n",
        "    activation=\"relu\",\n",
        "    strides=3\n",
        ")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# Vanilla hidden layer:\n",
        "x = layers.Dense(unidades, activation=\"relu\")(x)\n",
        "x = layers.Dropout(drop)(x)\n",
        "\n",
        "# Salida\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model_conv1d = tf.keras.Model(inputs, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DAyGrZtZ8z84",
        "outputId": "9923ac56-389b-476b-a2be-316174d0ea8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.6660 - accuracy: 0.5346 - val_loss: 0.7162 - val_accuracy: 0.5066\n",
            "Epoch 2/3\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6441 - accuracy: 0.5443 - val_loss: 0.7647 - val_accuracy: 0.5084\n",
            "Epoch 3/3\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6425 - accuracy: 0.5546 - val_loss: 0.7318 - val_accuracy: 0.5110\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f01c41f0970>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model_conv1d.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G6oMcQ7o8z85",
        "outputId": "1b8c92d7-b43f-46dd-d030-f7e542a74ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 3ms/step - loss: 0.6932 - accuracy: 0.4979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6932401061058044, 0.4979200065135956]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model_conv1d.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN5B_pXH8z85"
      },
      "source": [
        "## Modelo para producción\n",
        "\n",
        "Si ya tenemos nuestro modelo funcionando, y nos gusta, y queremos dejarlo en un formato que permita aplicarlo a los datos en crudo, es necesario empaquetar todo nuestro procedimiento en un solo procedimiento de principio a fin.\n",
        "\n",
        "Agregamos aqui el truco para empaqetar todo, cuando ya no se espera reentrenar el modelo (al menos no en el corto plazo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Le-5dXY58z85"
      },
      "outputs": [],
      "source": [
        "modelo_seleccionado = model_bi\n",
        "\n",
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "\n",
        "# Turn vocab indices into predictions\n",
        "outputs = modelo_seleccionado(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "end_to_end_model.save('nombre_codigo.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHAKqIp9930I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}